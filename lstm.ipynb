{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on NLP From Scratch: Generating Names with a Character-Level RNN\n",
    "*************************************************************\n",
    "by: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, bias=True, batch_first=False,\n",
    "                dropout=0, bidirectional=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, bias=bias, batch_first=batch_first,\n",
    "                           dropout=dropout, bidirectional=bidirectional)\n",
    "#         self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "#         self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "\n",
    "        self.o2o = nn.Linear(self.num_directions * hidden_size, output_size)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, my_input, hidden):\n",
    "#         input_combined = torch.cat((category, input, hidden), 1)\n",
    "#         hidden = self.i2h(input_combined)\n",
    "#         output = self.i2o(input_combined)\n",
    "#         output_combined = torch.cat((hidden, output), 1)\n",
    "#         output = self.o2o(output_combined)\n",
    "#         output = self.dropout(output)\n",
    "        output, hidden = self.lstm(my_input, hidden)\n",
    "        output = self.o2o(output)\n",
    "#         output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch=1, device=torch.device('cpu')):\n",
    "        return (torch.zeros(self.num_layers,1,self.hidden_size, device=device), torch.zeros(self.num_layers * self.num_directions,batch,self.hidden_size,device=device))\n",
    "#         return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def string_to_chars(original):\n",
    "    raw_lines = original.splitlines()\n",
    "\n",
    "    def proc_line(l): return [l] if l == '<start>' or l == '<end>' else list(l)\n",
    "    chars = reduce(lambda x, y : x + ['\\n'] + y, list(map(proc_line, raw_lines)))\n",
    "    assert original == ''.join(chars)\n",
    "    return chars\n",
    "\n",
    "def make_encoder(raw):\n",
    "    chars = string_to_chars(raw)\n",
    "    decoder = [c for c in Counter(chars)]\n",
    "    decoder = sorted(decoder)\n",
    "    return {c:i for i, c in enumerate(decoder)}, decoder\n",
    "\n",
    "def split_songs(raw):\n",
    "    return re.findall('(<start>.*?<end>)',raw,flags=re.DOTALL)\n",
    "\n",
    "# train_songs = list(map(string_to_chars, split_songs(raw_train)))\n",
    "\n",
    "# reconstructed = '\\n'.join(map(''.join, train_songs))\n",
    "# encoder, decoder = make_encoder(raw_train)\n",
    "# n_chars = len(decoder)\n",
    "\n",
    "def song_to_tensor_chunks(song,encoder, decoder):\n",
    "    training_chunks = []\n",
    "    target_chunks = []\n",
    "    for i in range(0,len(song),100):\n",
    "        start = i\n",
    "        if start+100 > len(song) - 1:\n",
    "            end = len(song)\n",
    "            target_range = list(np.arange(start + 1, end,dtype=np.int)) + [int(end - 1)]\n",
    "        else:\n",
    "            end = start + 100\n",
    "            target_range = np.arange(start+1, end+1, dtype=np.int)\n",
    "        chunk = torch.zeros(end-start, 1, n_chars)\n",
    "        for i, c in enumerate(song[start:end]):\n",
    "            chunk[i,0,encoder[c]] = 1\n",
    "        target_chunk = []\n",
    "        for i in target_range:\n",
    "            target_chunk.append(encoder[song[i]])\n",
    "        target_chunks.append(torch.tensor(target_chunk))\n",
    "        training_chunks.append(chunk)\n",
    "    return training_chunks, target_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = open('train.txt').read()\n",
    "train_songs = list(map(string_to_chars, split_songs(raw_train)))\n",
    "\n",
    "reconstructed = '\\n'.join(map(''.join, train_songs))\n",
    "encoder, decoder = make_encoder(raw_train)\n",
    "n_chars = len(decoder)\n",
    "val_songs = list(map(string_to_chars, split_songs(open('val.txt').read())))\n",
    "test_songs = list(map(string_to_chars, split_songs(open('test.txt').read())))\n",
    "assert reconstructed == raw_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience during training we'll make a ``randomTrainingExample``\n",
    "function that fetches a random (category, line) pair and turns them into\n",
    "the required (category, input, target) tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network\n",
    "--------------------\n",
    "\n",
    "In contrast to classification, where only the last output is used, we\n",
    "are making a prediction at every step, so we are calculating loss at\n",
    "every step.\n",
    "\n",
    "The magic of autograd allows you to simply sum these losses at each step\n",
    "and call backward at the end.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(song,optimizer=None, device=torch.device('cpu')):\n",
    "    train_chunks, target_chunks = song_to_tensor_chunks(song, encoder,decoder)\n",
    "    hidden = rnn.initHidden(device=device)\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for input_line_tensor, target_line_tensor in zip(train_chunks,target_chunks):\n",
    "        target_line_tensor.unsqueeze_(-1)\n",
    "        for i in range(input_line_tensor.size(0)):\n",
    "            output, hidden = rnn(input_line_tensor[i].view((1,1,n_chars)).to(device), hidden)\n",
    "    #         print(output)\n",
    "            l = criterion(output.view((1,n_chars)), target_line_tensor[i].to(device))\n",
    "            loss += l / input_line_tensor.size(0)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item() / len(train_chunks)\n",
    "def test(rnn, song, device=torch.device('cpu')):\n",
    "    train_chunks, target_chunks = song_to_tensor_chunks(song, encoder,decoder)\n",
    "    hidden = rnn.initHidden(device=device)\n",
    "    loss = 0\n",
    "    for input_line_tensor, target_line_tensor in zip(train_chunks,target_chunks):\n",
    "        target_line_tensor.unsqueeze_(-1)\n",
    "        output, hidden = rnn(input_line_tensor.to(device), hidden)\n",
    "        seq_len,_ =target_line_tensor.size()\n",
    "        l = criterion(output.view((seq_len,n_chars)), target_line_tensor.view((seq_len)).to(device))\n",
    "        loss += l\n",
    "    return output, loss.item() / len(train_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_chars, 100, n_chars).to(device) #hidden_size originally 128\n",
    "#rnn.load_state_dict(torch.load('rnn_1574760411.pt'))\n",
    "rnn = rnn.to(device)\n",
    "#train_loss = list(map(float,(open('train_loss_1574760411.csv').read().splitlines())))\n",
    "#val_loss = list(map(float,(open('val_loss_1574760411.csv').read().splitlines())))\n",
    "#print(len(train_loss))\n",
    "#start = len(train_loss)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 0s (0 0%) 2.7532\n",
      "0m 46s (80 9%) 2.7193\n",
      "1m 36s (160 19%) 2.2925\n",
      "2m 27s (240 29%) 3.3124\n",
      "3m 13s (320 39%) 2.5487\n",
      "4m 6s (400 49%) 2.6033\n",
      "4m 58s (480 59%) 3.1760\n",
      "5m 49s (560 69%) 2.5699\n",
      "6m 48s (640 79%) 2.3490\n",
      "7m 39s (720 89%) 2.6861\n",
      "8m 29s (800 99%) 2.5357\n",
      "Epoch 1574860735 8m 34s val loss: 2.2785 train loss: 2.5367\n",
      "\n",
      "8m 34s (0 0%) 2.4826\n",
      "9m 30s (80 9%) 2.3223\n",
      "10m 24s (160 19%) 2.4637\n",
      "11m 16s (240 29%) 1.9669\n",
      "12m 8s (320 39%) 1.8576\n",
      "13m 1s (400 49%) 2.4203\n",
      "13m 46s (480 59%) 2.1977\n",
      "14m 31s (560 69%) 1.5729\n",
      "15m 20s (640 79%) 2.2863\n",
      "16m 11s (720 89%) 2.4051\n",
      "17m 2s (800 99%) 2.1248\n",
      "Epoch 1574860736 17m 7s val loss: 2.1230 train loss: 2.1909\n",
      "\n",
      "17m 7s (0 0%) 2.3153\n",
      "17m 54s (80 9%) 2.0813\n",
      "18m 49s (160 19%) 2.6668\n",
      "19m 35s (240 29%) 1.9436\n",
      "20m 26s (320 39%) 2.4508\n",
      "21m 17s (400 49%) 2.1246\n",
      "22m 7s (480 59%) 1.9143\n",
      "23m 2s (560 69%) 1.7308\n",
      "23m 48s (640 79%) 1.6070\n",
      "24m 37s (720 89%) 1.0877\n",
      "25m 36s (800 99%) 2.0857\n",
      "Epoch 1574860737 25m 40s val loss: 1.9795 train loss: 1.9923\n",
      "\n",
      "25m 40s (0 0%) 2.1496\n",
      "26m 28s (80 9%) 1.9500\n",
      "27m 20s (160 19%) 1.5869\n",
      "28m 10s (240 29%) 2.2181\n",
      "29m 6s (320 39%) 1.7616\n",
      "30m 0s (400 49%) 1.7072\n",
      "30m 53s (480 59%) 1.9274\n",
      "31m 45s (560 69%) 2.3548\n",
      "32m 37s (640 79%) 1.7722\n",
      "33m 30s (720 89%) 1.6094\n",
      "34m 23s (800 99%) 1.7274\n",
      "Epoch 1574860738 34m 26s val loss: 1.9024 train loss: 1.8468\n",
      "\n",
      "34m 26s (0 0%) 1.6050\n",
      "35m 16s (80 9%) 1.8101\n",
      "36m 5s (160 19%) 1.4839\n",
      "36m 56s (240 29%) 2.1069\n",
      "37m 45s (320 39%) 1.7006\n",
      "38m 34s (400 49%) 1.9115\n",
      "39m 34s (480 59%) 1.4267\n",
      "40m 26s (560 69%) 1.8436\n",
      "41m 17s (640 79%) 1.3916\n",
      "42m 8s (720 89%) 2.3548\n",
      "43m 2s (800 99%) 1.9466\n",
      "Epoch 1574860739 43m 6s val loss: 1.7694 train loss: 1.7453\n",
      "\n",
      "43m 6s (0 0%) 1.9932\n",
      "43m 56s (80 9%) 1.8256\n",
      "44m 52s (160 19%) 1.5060\n",
      "45m 42s (240 29%) 1.5451\n",
      "46m 35s (320 39%) 2.1483\n",
      "47m 30s (400 49%) 1.3732\n",
      "48m 20s (480 59%) 1.6958\n",
      "49m 11s (560 69%) 1.3323\n",
      "49m 51s (640 79%) 1.4919\n",
      "50m 28s (720 89%) 1.4678\n",
      "51m 11s (800 99%) 1.5953\n",
      "Epoch 1574860740 51m 16s val loss: 1.7787 train loss: 1.6639\n",
      "\n",
      "51m 17s (0 0%) 1.5424\n",
      "52m 9s (80 9%) 1.7416\n",
      "52m 54s (160 19%) 1.7575\n",
      "53m 46s (240 29%) 1.3507\n",
      "54m 39s (320 39%) 1.7037\n",
      "55m 35s (400 49%) 1.4042\n",
      "56m 28s (480 59%) 1.7054\n",
      "57m 14s (560 69%) 2.4473\n",
      "58m 5s (640 79%) 1.2411\n",
      "58m 54s (720 89%) 1.7153\n",
      "59m 42s (800 99%) 1.6093\n",
      "Epoch 1574860741 59m 46s val loss: 1.7034 train loss: 1.6005\n",
      "\n",
      "59m 47s (0 0%) 1.4519\n",
      "60m 40s (80 9%) 1.7448\n",
      "61m 35s (160 19%) 1.6747\n",
      "62m 21s (240 29%) 1.5713\n",
      "63m 11s (320 39%) 1.9847\n",
      "63m 56s (400 49%) 1.3639\n",
      "64m 38s (480 59%) 1.1891\n",
      "65m 23s (560 69%) 1.4466\n",
      "66m 13s (640 79%) 1.3224\n",
      "67m 14s (720 89%) 1.3510\n",
      "68m 5s (800 99%) 1.2750\n",
      "Epoch 1574860742 68m 9s val loss: 1.6821 train loss: 1.5464\n",
      "\n",
      "68m 10s (0 0%) 1.3397\n",
      "68m 59s (80 9%) 1.3791\n",
      "69m 43s (160 19%) 2.1529\n",
      "70m 35s (240 29%) 1.7132\n",
      "71m 22s (320 39%) 1.5304\n",
      "72m 15s (400 49%) 1.6945\n",
      "72m 55s (480 59%) 1.3497\n",
      "73m 44s (560 69%) 2.3217\n",
      "74m 30s (640 79%) 1.3570\n",
      "75m 22s (720 89%) 1.2682\n",
      "76m 13s (800 99%) 1.4809\n",
      "Epoch 1574860743 76m 16s val loss: 1.6754 train loss: 1.5034\n",
      "\n",
      "76m 17s (0 0%) 1.2568\n",
      "77m 10s (80 9%) 1.7428\n",
      "77m 49s (160 19%) 1.5525\n",
      "78m 39s (240 29%) 1.4434\n",
      "79m 38s (320 39%) 1.0224\n",
      "80m 31s (400 49%) 1.4788\n",
      "81m 17s (480 59%) 1.7806\n",
      "82m 5s (560 69%) 1.3060\n",
      "82m 57s (640 79%) 1.7468\n",
      "83m 50s (720 89%) 1.5015\n",
      "84m 34s (800 99%) 1.9110\n",
      "Epoch 1574860744 84m 38s val loss: 1.6400 train loss: 1.4746\n",
      "\n",
      "84m 39s (0 0%) 1.8590\n",
      "85m 32s (80 9%) 1.3638\n",
      "86m 28s (160 19%) 1.2707\n",
      "87m 26s (240 29%) 2.2862\n",
      "88m 20s (320 39%) 1.1792\n",
      "89m 17s (400 49%) 1.2985\n",
      "90m 13s (480 59%) 0.8646\n",
      "91m 8s (560 69%) 1.7622\n",
      "92m 0s (640 79%) 1.0965\n",
      "92m 46s (720 89%) 1.2988\n",
      "93m 38s (800 99%) 1.4916\n",
      "Epoch 1574860745 93m 42s val loss: 1.6139 train loss: 1.4320\n",
      "\n",
      "93m 43s (0 0%) 1.2711\n",
      "94m 29s (80 9%) 1.0024\n",
      "95m 14s (160 19%) 2.0362\n",
      "96m 2s (240 29%) 0.9966\n",
      "96m 52s (320 39%) 1.2106\n",
      "97m 40s (400 49%) 1.3829\n",
      "98m 29s (480 59%) 1.9872\n",
      "99m 25s (560 69%) 1.0011\n",
      "100m 20s (640 79%) 1.8636\n",
      "101m 13s (720 89%) 1.6104\n",
      "102m 12s (800 99%) 2.0861\n",
      "Epoch 1574860746 102m 17s val loss: 1.5897 train loss: 1.4067\n",
      "\n",
      "102m 17s (0 0%) 1.6229\n",
      "103m 5s (80 9%) 0.7308\n",
      "104m 2s (160 19%) 1.3828\n",
      "104m 48s (240 29%) 1.2830\n",
      "105m 39s (320 39%) 1.5937\n",
      "106m 27s (400 49%) 1.1820\n",
      "107m 15s (480 59%) 1.2752\n",
      "108m 15s (560 69%) 1.4589\n",
      "109m 7s (640 79%) 1.2769\n",
      "109m 58s (720 89%) 0.6797\n",
      "110m 50s (800 99%) 1.4302\n",
      "Epoch 1574860747 110m 54s val loss: 1.5785 train loss: 1.3842\n",
      "\n",
      "110m 54s (0 0%) 1.1239\n",
      "111m 45s (80 9%) 1.2471\n",
      "112m 41s (160 19%) 1.5365\n",
      "113m 32s (240 29%) 0.8413\n",
      "114m 19s (320 39%) 1.3423\n",
      "115m 6s (400 49%) 1.1508\n",
      "115m 56s (480 59%) 1.6399\n",
      "116m 45s (560 69%) 1.3309\n",
      "117m 44s (640 79%) 1.2324\n",
      "118m 35s (720 89%) 1.3277\n",
      "119m 25s (800 99%) 1.1621\n",
      "Epoch 1574860748 119m 32s val loss: 1.5827 train loss: 1.3577\n",
      "\n",
      "119m 32s (0 0%) 1.4111\n",
      "120m 29s (80 9%) 1.6050\n",
      "121m 19s (160 19%) 0.8485\n",
      "122m 10s (240 29%) 1.1221\n",
      "123m 0s (320 39%) 1.1302\n",
      "123m 49s (400 49%) 1.4041\n",
      "124m 42s (480 59%) 1.3340\n",
      "125m 32s (560 69%) 1.2313\n",
      "126m 24s (640 79%) 1.2927\n",
      "127m 20s (720 89%) 1.3702\n",
      "128m 12s (800 99%) 1.5174\n",
      "Epoch 1574860749 128m 16s val loss: 1.5794 train loss: 1.3408\n",
      "\n",
      "128m 16s (0 0%) 1.6961\n",
      "129m 6s (80 9%) 1.3938\n",
      "130m 1s (160 19%) 1.4781\n",
      "130m 52s (240 29%) 1.6888\n",
      "131m 46s (320 39%) 1.2204\n",
      "132m 38s (400 49%) 1.0238\n",
      "133m 25s (480 59%) 1.1148\n",
      "134m 12s (560 69%) 0.9873\n",
      "135m 5s (640 79%) 1.1555\n",
      "135m 55s (720 89%) 1.2967\n",
      "136m 49s (800 99%) 1.3407\n",
      "Epoch 1574860750 136m 54s val loss: 1.5617 train loss: 1.3196\n",
      "\n",
      "136m 54s (0 0%) 1.3528\n",
      "137m 55s (80 9%) 1.2951\n",
      "138m 45s (160 19%) 1.2498\n",
      "139m 32s (240 29%) 1.1598\n",
      "140m 29s (320 39%) 1.4211\n",
      "141m 22s (400 49%) 1.1771\n",
      "142m 11s (480 59%) 1.1223\n",
      "142m 58s (560 69%) 1.1838\n",
      "143m 52s (640 79%) 1.1301\n",
      "144m 43s (720 89%) 1.5689\n",
      "145m 31s (800 99%) 1.4641\n",
      "Epoch 1574860751 145m 36s val loss: 1.5418 train loss: 1.3022\n",
      "\n",
      "145m 36s (0 0%) 1.1732\n",
      "146m 30s (80 9%) 1.3487\n",
      "147m 18s (160 19%) 1.2913\n",
      "147m 58s (240 29%) 1.7785\n",
      "148m 52s (320 39%) 1.1706\n",
      "149m 35s (400 49%) 0.9288\n",
      "150m 30s (480 59%) 0.8618\n",
      "151m 21s (560 69%) 1.1609\n",
      "152m 13s (640 79%) 1.5457\n",
      "153m 5s (720 89%) 1.2434\n",
      "153m 56s (800 99%) 1.1297\n",
      "Epoch 1574860752 154m 0s val loss: 1.5582 train loss: 1.2889\n",
      "\n",
      "154m 1s (0 0%) 1.2217\n",
      "154m 57s (80 9%) 1.4132\n",
      "155m 51s (160 19%) 1.1942\n",
      "156m 35s (240 29%) 1.0825\n",
      "157m 24s (320 39%) 0.9714\n",
      "158m 22s (400 49%) 1.3393\n",
      "159m 17s (480 59%) 1.3405\n",
      "160m 7s (560 69%) 0.8476\n",
      "160m 59s (640 79%) 1.1914\n",
      "161m 49s (720 89%) 1.0162\n",
      "162m 43s (800 99%) 1.5357\n",
      "Epoch 1574860753 162m 47s val loss: 1.5448 train loss: 1.2707\n",
      "\n",
      "162m 47s (0 0%) 1.2862\n",
      "163m 40s (80 9%) 1.4037\n",
      "164m 33s (160 19%) 1.1691\n",
      "165m 22s (240 29%) 1.3048\n",
      "166m 12s (320 39%) 0.8004\n",
      "166m 59s (400 49%) 0.9805\n",
      "167m 47s (480 59%) 1.9573\n",
      "168m 42s (560 69%) 0.6698\n",
      "169m 31s (640 79%) 1.2628\n",
      "170m 19s (720 89%) 1.5075\n",
      "171m 16s (800 99%) 1.2180\n",
      "Epoch 1574860754 171m 18s val loss: 1.5535 train loss: 1.2603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rnn = RNN(n_chars, 128, n_chars).to(device)\n",
    "learning_rate = 0.05\n",
    "# optimizer = torch.optim.Adam(rnn.parameters())\n",
    "n_iters = 10000\n",
    "print_every = 80\n",
    "plot_every = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "n_epochs = 20\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(int(start),int(start+n_epochs)):\n",
    "    shuffle(train_songs)\n",
    "    total_loss = 0\n",
    "    for i, song in enumerate(train_songs):\n",
    "        output, loss = train(song,optimizer=optimizer, device=device)\n",
    "        total_loss += loss\n",
    "        if i % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), i, i / len(train_songs) * 100, loss))\n",
    "\n",
    "\n",
    "    train_loss.append(total_loss / len(train_songs))\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for song in test_songs:\n",
    "            _, loss = test(rnn, song, device=device)\n",
    "            total_loss += loss\n",
    "        val_loss.append(total_loss / len(test_songs))\n",
    "    torch.save(rnn.state_dict(),f'rnn_{epoch}.pt')\n",
    "    with open(f'train_loss_{epoch}.csv', 'w+') as out:\n",
    "        out.write('\\n'.join(map(str,train_loss)))\n",
    "\n",
    "    with open(f'val_loss_{epoch}.csv', 'w+') as out:\n",
    "        out.write('\\n'.join(map(str,val_loss)))\n",
    "    print('Epoch %d %s val loss: %.4f train loss: %.4f\\n' % (epoch,timeSince(start), val_loss[-1], train_loss[-1]))   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5367129308997427, 2.190908712812478, 1.9923137943961395, 1.846779023665649, 1.7452958142785664, 1.6638741107951065, 1.600470591454109, 1.546412259929671, 1.5034050571506863, 1.4745895855487376, 1.4320434205136054, 1.406719924045259, 1.3842393386963041, 1.3576616240824781, 1.3408265668992454, 1.3196220277680806, 1.3022022568206173, 1.2888662287889237, 1.2707248890103247, 1.2603410774352026]\n",
      "[2.2785263149579507, 2.122958825581485, 1.9795486480272244, 1.9024232473531768, 1.769352910131801, 1.7786977130739874, 1.7033790068161916, 1.6820820049526435, 1.6753724007873156, 1.6399661394319145, 1.6139073459195927, 1.589667223520683, 1.5784670147419029, 1.582696457402064, 1.579413537628565, 1.561722125328431, 1.5418219784571194, 1.5581586753418462, 1.5447819750654515, 1.5535016826768067]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "offset = 0\n",
    "epoch_nums = [i for i in range(len(train_loss)+1)]\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "with open('train_loss_{offset}.csv', 'w+') as out:\n",
    "    out.write('\\n'.join(map(str,train_loss)))\n",
    "\n",
    "with open('val_loss_{offset}.csv', 'w+') as out:\n",
    "    out.write('\\n'.join(map(str,val_loss)))\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X: 15\n",
      "T:Altar Boule\n",
      "R:hornpipe\n",
      "H:Saile de ceenbro: Mabareduro Ire #13\n",
      "Z:id:hn-hornpipe-15\n",
      "M:C|\n",
      "K:b\n",
      "P:B\n",
      "V:Brub\n",
      "F2G AFE | B2B2 (c/cd e ef dB dc B>d d>g e>d ef dB dc | e2 d>d fe f>f fd>B  dBd cBc d2 d>d ef fe d2 d4 | d2fe g4 fe>f d>c B>d Bd c>d d>d| ed dB d>A c>d d>d | f2d B>d d=B/B/ | cc/d/ Bd/e/ | ef ef | f2 f>e d>f ed | \n",
      "g f>d ga/g/ eb b2 f>d c> B>A FE E2 F2 GA \"D/G/A/B//2A/ G/2G/2 AG/G/ GEE:|\n",
      "|:haf fd/f/ g/g/ a gf | ea g/f/d/A/ | g>a bg | \n",
      "gf g>d cB | d2 d2 d>d Bc de | ed cd e>f c>d e2 cd d>e fd/G/ de ga | da f>g fz ||\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "# Generates song using temperature of samples\n",
    "def sample_temp(temperature = 0.8):\n",
    "    with torch.no_grad():\n",
    "        input_tensor_chunk, _ = song_to_tensor_chunks(['<start>'], encoder, decoder)\n",
    "        input_tensor = input_tensor_chunk[0].to(device)\n",
    "        hidden = rnn.initHidden(device=device)\n",
    "        prev_char_raw, hidden = rnn(input_tensor,hidden) \n",
    "        prev_char_probs = torch.softmax(prev_char_raw, 2, prev_char_raw.dtype).view((n_chars))\n",
    "        output = '<start>'\n",
    "        while True:\n",
    "            output_dist = prev_char_raw.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            selected_char = decoder[top_i.item()]\n",
    "            output += selected_char\n",
    "            input_tensor_chunk, _ = song_to_tensor_chunks([selected_char], encoder, decoder)\n",
    "            input_tensor = input_tensor_chunk[0].to(device)\n",
    "            # End song \n",
    "            if selected_char == '<end>':\n",
    "                break\n",
    "            # Get next letter \n",
    "            else:\n",
    "                prev_char_raw, hidden = rnn(input_tensor,hidden) \n",
    "                prev_char_probs = torch.softmax(prev_char_raw, 2, prev_char_raw.dtype).view((n_chars))\n",
    "        return output\n",
    "print(sample_temp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_char \n",
      "\n",
      "selected_char X\n",
      "selected_char :\n",
      "selected_char 1\n",
      "selected_char \n",
      "\n",
      "selected_char T\n",
      "selected_char :\n",
      "selected_char B\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char r\n",
      "selected_char  \n",
      "selected_char t\n",
      "selected_char h\n",
      "selected_char e\n",
      "selected_char  \n",
      "selected_char o\n",
      "selected_char b\n",
      "selected_char s\n",
      "selected_char e\n",
      "selected_char  \n",
      "selected_char t\n",
      "selected_char h\n",
      "selected_char e\n",
      "selected_char  \n",
      "selected_char C\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char r\n",
      "selected_char n\n",
      "selected_char e\n",
      "selected_char  \n",
      "selected_char T\n",
      "selected_char h\n",
      "selected_char e\n",
      "selected_char \n",
      "\n",
      "selected_char R\n",
      "selected_char :\n",
      "selected_char h\n",
      "selected_char o\n",
      "selected_char r\n",
      "selected_char n\n",
      "selected_char p\n",
      "selected_char i\n",
      "selected_char p\n",
      "selected_char e\n",
      "selected_char \n",
      "\n",
      "selected_char C\n",
      "selected_char :\n",
      "selected_char T\n",
      "selected_char r\n",
      "selected_char a\n",
      "selected_char d\n",
      "selected_char .\n",
      "selected_char \n",
      "\n",
      "selected_char Z\n",
      "selected_char :\n",
      "selected_char T\n",
      "selected_char r\n",
      "selected_char a\n",
      "selected_char n\n",
      "selected_char s\n",
      "selected_char c\n",
      "selected_char r\n",
      "selected_char i\n",
      "selected_char t\n",
      "selected_char  \n",
      "selected_char e\n",
      "selected_char t\n",
      "selected_char /\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char  \n",
      "selected_char c\n",
      "selected_char o\n",
      "selected_char r\n",
      "selected_char r\n",
      "selected_char i\n",
      "selected_char g\n",
      "selected_char ?\n",
      "selected_char  \n",
      "selected_char p\n",
      "selected_char a\n",
      "selected_char r\n",
      "selected_char  \n",
      "selected_char M\n",
      "selected_char i\n",
      "selected_char c\n",
      "selected_char h\n",
      "selected_char e\n",
      "selected_char l\n",
      "selected_char  \n",
      "selected_char B\n",
      "selected_char E\n",
      "selected_char L\n",
      "selected_char L\n",
      "selected_char O\n",
      "selected_char N\n",
      "selected_char  \n",
      "selected_char -\n",
      "selected_char  \n",
      "selected_char 2\n",
      "selected_char 0\n",
      "selected_char 0\n",
      "selected_char 5\n",
      "selected_char -\n",
      "selected_char 0\n",
      "selected_char 7\n",
      "selected_char -\n",
      "selected_char 2\n",
      "selected_char 3\n",
      "selected_char \n",
      "\n",
      "selected_char Z\n",
      "selected_char :\n",
      "selected_char P\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char r\n",
      "selected_char  \n",
      "selected_char t\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char t\n",
      "selected_char e\n",
      "selected_char  \n",
      "selected_char o\n",
      "selected_char b\n",
      "selected_char s\n",
      "selected_char e\n",
      "selected_char r\n",
      "selected_char v\n",
      "selected_char a\n",
      "selected_char t\n",
      "selected_char i\n",
      "selected_char o\n",
      "selected_char n\n",
      "selected_char  \n",
      "selected_char m\n",
      "selected_char a\n",
      "selected_char i\n",
      "selected_char l\n",
      "selected_char t\n",
      "selected_char o\n",
      "selected_char :\n",
      "selected_char g\n",
      "selected_char a\n",
      "selected_char l\n",
      "selected_char o\n",
      "selected_char u\n",
      "selected_char v\n",
      "selected_char i\n",
      "selected_char e\n",
      "selected_char l\n",
      "selected_char l\n",
      "selected_char e\n",
      "selected_char @\n",
      "selected_char f\n",
      "selected_char r\n",
      "selected_char e\n",
      "selected_char e\n",
      "selected_char .\n",
      "selected_char f\n",
      "selected_char r\n",
      "selected_char \n",
      "\n",
      "selected_char M\n",
      "selected_char :\n",
      "selected_char 2\n",
      "selected_char /\n",
      "selected_char 4\n",
      "selected_char \n",
      "\n",
      "selected_char L\n",
      "selected_char :\n",
      "selected_char 1\n",
      "selected_char /\n",
      "selected_char 8\n",
      "selected_char \n",
      "\n",
      "selected_char K\n",
      "selected_char :\n",
      "selected_char G\n",
      "selected_char \n",
      "\n",
      "selected_char G\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char |\n",
      "selected_char  \n",
      "selected_char B\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char B\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char B\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char |\n",
      "selected_char  \n",
      "selected_char B\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char 2\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char e\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "selected_char d\n",
      "selected_char >\n",
      "selected_char d\n",
      "selected_char  \n",
      "<start>\n",
      "X:1\n",
      "T:Bour the obse the Courne The\n",
      "R:hornpipe\n",
      "C:Trad.\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-23\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:G\n",
      "G2 | B2 B2 B2 | B2 d2 d>d ed d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d d>d <end>\n"
     ]
    }
   ],
   "source": [
    "def sample_max(max_length = 1000):\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        input_tensor_chunk, _ = song_to_tensor_chunks(['<start>'], encoder, decoder)\n",
    "        input_tensor = input_tensor_chunk[0].to(device)\n",
    "        hidden = rnn.initHidden(device=device)\n",
    "        prev_char_raw, hidden = rnn(input_tensor,hidden) \n",
    "        output = '<start>'\n",
    "        maxarg = torch.argmax(prev_char_raw)\n",
    "        while True:\n",
    "            selected_char = decoder[maxarg.item()]\n",
    "            print('selected_char', selected_char)\n",
    "            output += selected_char\n",
    "            input_tensor_chunk, _ = song_to_tensor_chunks([selected_char], encoder, decoder)\n",
    "            input_tensor = input_tensor_chunk[0].to(device)\n",
    "            # End of song\n",
    "            if selected_char == '<end>':\n",
    "                break\n",
    "            # Song is repeating and wont end\n",
    "            if counter == max_length:\n",
    "                output += '<end>'\n",
    "                break\n",
    "            else:\n",
    "                prev_char_raw, hidden = rnn(input_tensor,hidden) \n",
    "                maxarg = torch.argmax(prev_char_raw)\n",
    "            counter += 1\n",
    "        return output\n",
    "print(sample_max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:15\n",
      "T:Trighe\n",
      "T:Hhedfgiad lacle Toumued\", The The Steatrigy The\n",
      "R:polka\n",
      "Z:id:hn-hornppe-14-18\n",
      "M:6/8\n",
      "K:G\n",
      "GcB dBBG|dcBc A2Bc||\n",
      "d(dcA cBA Bdfe ||\n",
      "AcdB ABAB | BcBd gfe fe | e4 d2e dge | B2 B2 B2 BBG | c2A B2 A>d de d2 dcB GF GA G>A cdB | d3 d2 ed | e2d B3 dBd | c2 (f2e/)de !+!dor\n",
      "ge de cA de f2 dB dc Ad ec dc d2 d>d | ed ec dd | ag d>d AB dc fe | ed B3 (3/2z/) gg | ed B2 c>e d>d c2 | d2 A2 z>A B>A d>B FF E>F ED | G2 FA AA BB cc | d2g e>d fe | ef ed d>d dc | d>d dc B/d==e d>e !breeth! d2 :|\n",
      "<end>\n",
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:461\n",
      "T:Shanched, The\n",
      "T:Sea Franond\n",
      "T:Farantorn:The\n",
      "R:marche\n",
      "C:Ju- Grollan ted obe \"Overmo Mariger\n",
      "Z:id:hn-he-27\n",
      "M:6/8\n",
      "K:G\n",
      "AD AcB | (3GAB Bc ded | fed ecd e2g | dfe dBAG | FBc FEG ABc | d2 d2 cdB | BGF Dg/f/e/ fg | d3 ef c2 :|\n",
      "<end>\n",
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:6\n",
      "T:Hire gander\n",
      "R:hornpip\n",
      "Z:id:hn-jig-64\n",
      "M:C|\n",
      "K:D\n",
      "~B3|eBA cBA|ABc Bccd|cBg efe|ddc BABG|A2AG BAGF|EGAB cAGB|1 BGA GBc | BAF B2AB|dBcc d2DE||\n",
      "<end>\n",
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:2\n",
      "T:Hallay A Thampa, The\n",
      "R:jig\n",
      "Z:id:hn-jig-71\n",
      "M:6/8\n",
      "K:Am\n",
      "(f2c dd =fef | edB c>ef dc2 | B2B2 A2E2 | E4 B2 F4 | FG E2 EEF | FF cB BGA | B2 G2 E>B | A2 dB GB AB d2 Bc B2 A>A BA GF G2 AB | (3/2)B cd/2 d2 GB dc/2>/ (3B/2G/2B/2 G/2B/A/)G | G3/2B/2B/2 B/2B/2B/2 B/2d/c/B/2 Bd/2f/3/22 d>d2e/2.2..br\n",
      "c.|f2 g2f/2g/2 e2 c/2d/2B/2d/2d/2 !.4...5-! |]\n",
      "<end>\n",
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:37\n",
      "T:alighiong O'cary Gintyhy\n",
      "R:polka\n",
      "H:Al=pest Iait le do par que tin the Dy\n",
      "D:Seb aillo ta Bartinis Lh Jige\n",
      "Z:id:hn-hornpipe\n",
      "M:C||\n",
      "K:G\n",
      "|:|\n",
      "|:|2\n",
      "d2Bc BdBc|\n",
      "A2ef gge |fdB d2e>f|ggg a4|e2dB B3:|\n",
      "<end>\n",
      "tensor([4.2465e-08, 9.9973e-01, 2.0727e-05, 1.6488e-07, 4.9838e-07, 1.8923e-07,\n",
      "        3.4205e-08, 4.0373e-07, 1.0571e-06, 1.5788e-06, 3.0465e-08, 7.2443e-08,\n",
      "        2.2444e-06, 2.5675e-06, 2.0363e-06, 4.0226e-07, 2.0947e-05, 1.0290e-04,\n",
      "        2.2070e-06, 5.8563e-07, 9.4304e-07, 2.6823e-05, 1.3309e-05, 6.8630e-06,\n",
      "        8.9454e-07, 7.3722e-06, 3.9822e-06, 3.8618e-08, 3.1200e-09, 7.5412e-10,\n",
      "        3.4363e-07, 2.0964e-07, 8.2835e-07, 9.2270e-08, 1.6127e-07, 8.0383e-07,\n",
      "        4.3177e-07, 9.8455e-07, 4.7658e-07, 3.8624e-07, 6.8874e-07, 7.2042e-08,\n",
      "        4.1709e-07, 1.2190e-07, 1.3115e-07, 1.2197e-07, 3.2569e-07, 1.0048e-08,\n",
      "        7.4366e-09, 1.0547e-07, 4.2875e-08, 1.1428e-08, 2.8098e-08, 2.2973e-07,\n",
      "        4.1117e-08, 5.3874e-08, 2.8445e-08, 8.9403e-06, 2.0076e-08, 1.8460e-08,\n",
      "        1.5638e-06, 2.1176e-06, 2.6705e-05, 7.4391e-08, 1.6207e-07, 1.4134e-07,\n",
      "        4.6817e-07, 1.3973e-07, 1.9878e-08, 8.7426e-08, 8.5638e-08, 6.0738e-08,\n",
      "        7.9807e-08, 1.5996e-07, 9.4598e-09, 1.5429e-08, 1.6615e-08, 4.6886e-08,\n",
      "        4.5542e-08, 6.4045e-08, 3.2994e-08, 7.9770e-08, 6.8708e-07, 4.1261e-07,\n",
      "        2.2178e-08, 3.0417e-07, 9.6212e-08, 5.2619e-08, 8.5982e-08, 4.5914e-08,\n",
      "        7.0245e-08, 4.3497e-08, 4.7448e-07, 1.5230e-09, 4.2178e-07],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "X:5\n",
      "T:Ferrolan's Connan's ETNian Reandhe\n",
      "R:hornpipe\n",
      "H:de bouth sastathersB\n",
      "Z:id:hn-jig-24\n",
      "M:2/4\n",
      "K:D\n",
      "L:1/8\n",
      "K:F\n",
      "(G2) A>A Bd B>d | e2f f>g d>d c>A de f>e ge | e2c d2 f>d ec de fg gag dd e2 df d>d eg | e2d gf/e/ e/d/e/d/ d>ded | c/ d/e/e/g/ ede | f/e/ e/c/c/d/ e/f/g/f/e/ b/ag/g/ ed c2 de | dg e>d ce BA AB/B/ G|: G/E/E/ A/G/G/ A/ B/A/B/G/ | AB BG | AG FA A>^c d>c Bc cA/A/A/ | ed/c/B/d/ Bc B/B/B/d/ | fe f>f d>c dc dD Bd | dc d/B/ AB/c/ | d2 cd eg/f/ | d3 ec/2c/B/ d/e/2 dB/c/ (3d//B/ e/d/ e/f/g/ ed ef dc de/d/ d/f/g/ d/B/G/B/ Ac (B/A/B/G/B/A/ B/A/F/B/G | AG Gz BA :|\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "# Normal run 20 epochs\n",
    "for i in range(6):\n",
    "    print(sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "X:60\n",
      "T:Framdo dercioner's(\n",
      "R:Scyledhe Heral S:C. (are.z.\n",
      "R:Scolka Ginthe\n",
      "Z:id:hn-polka-31\n",
      "M:2/4\n",
      "L:1/12\n",
      "K:G\n",
      "~G/e/ gG |\n",
      "<end>\n",
      "<start>\n",
      "X:9\n",
      "T:Lorow Roeutin, The\n",
      "R:jig\n",
      "L:1/8\n",
      "O:Frave\n",
      "C:hnDne-020\n",
      "K:GB|\n",
      "P:Finlendouto! d fff e2g | cdf d2 c3 | c>d cB c2 :| BF/B/c/B/ AB/e/ fe/f/f | f4 ga | e3 fc d dB Bdd/2 e/d/f/ gf e/d/d/e/ df ef gf f>f ed dc cA FF GA BBA AG | (G/c/B/d/ B/B/ BB |f/g/ eB g/e/c/e/ Bd/e/ ed gg | eg f>f fe | f>d ed de dd | de dc dB | de fe ee>f | g3 eb/c/ d/d>cB | B>G E>B A>B A| cd de ga f>e fe d>d g>e c | d2 gd/e/d/A/ GB cf B>d ce d>c>e ff>f f>d ff eg fg e>df G2 d>d | e2e fd/f/>d/2 g3 d>c A>Bc dc cB AB Bd c>ec | c/d/ d/e//e/d/A/ d>B AB BA | A2 AF BA F2 :|\n",
      "<end>\n",
      "<start>\n",
      "X:15\n",
      "T:Pour lourif's\n",
      "N:Cialle\n",
      "O:France\n",
      "C:Trag.\n",
      "E:Prougic.\n",
      "C:La gfithor pary ig\n",
      "Z:id:hn-march-31\n",
      "M:2/4\n",
      "L:1/8\n",
      "H:Aristtion: O, 98L\n",
      "D:wolliche -riesetalyu ttgan:\n",
      "Z:A/argale\n",
      "M:2/4\n",
      "|:26\n",
      "|: c/c/d/f/g/ d/B/ Bd df | ge/f/e/ d2 d | cd/c/ G2 B/2e/2 | B2z E>B/2G/ | E3/2G/4A/2 B3/4/2:| \n",
      "|:B2B/2 d/^2 dc/2c/2 :e2A2 GF AA>z | !!F+[c/=c/B/E/)B/ E2 | [cc/B/B/d/e/ c/B/c/c/ c/B/d/B/ | B>A_Az | cB cd/f/.\"Cm\"d.m!.f.f!A) g/f/d/B/.fc!!du!a>+b/d/B/B/ d/e/d/e/ fd |  dA/B/c/B/ c>d a g>g=e/2 cd/B/ .d/d/e/.d/>d d/dc/.d/d/ dc | f/2e/2 d/2/2e/2 B B/2c/2A/2B/4B2 Bc/2d/2d2 c2 AF | c>c>d2) | c6 :|2 d>d/2B/2 c6:|\n",
      "[K] w:\n",
      "dag/g/c | g2 aba | f2e2 d2f2 | e2 fg eB d2 eg/f/f/2 | baf!bad) g2-be d>d Bd>c B3A A3/2G/2/2B/2B/2c/2 B2 AABA)|\n",
      ".23/2G/2G/2A/2 AA c>z zd | \"8\"Gm\"B3 | \"F\"c>e BA)B>z| GA/A/B/d/ gf^d !+.F5 ||\n",
      "<end>\n",
      "<start>\n",
      "X:91\n",
      "T:Padhiraumpin\n",
      "T:Tlointne roino's Bully Lee Pimhe\n",
      "T:Szhot Hasctrte\n",
      "R:hornpipe\n",
      "Z:A:ichale sollofze wha?ut\n",
      "<end>\n",
      "<start>\n",
      "X:58\n",
      "T:Levantidn\n",
      "H:Ol theme rounes o, C\\'ianise -hea gamol'sp2tecke\n",
      "K:Fmajinlamo\n",
      "(9 (3B=c>d e3d.e dc/c/d/c/2 | e2dc>d2 e=bg.f[df | e4 gf/2d/2d/.d/.d.d4 .>fre!/2f2f/2e/2 f2 (B2e | c4 d2 \"Gm\"GG B|\"L\"A3 d2\"E\" e Le>c | d3d :|\n",
      "[K:Fb]G/C\"^G GAB/c/3 | A FF z>F F3 | G>d Ac/2d/2 ||\n",
      "<end>\n",
      "<start>\n",
      "X:69\n",
      "T:Laisc Branled, The\n",
      "R:Garnan.\n",
      "A:Provence\n",
      "C:Tradconnate\n",
      "M:C|\n",
      "L:1/8\n",
      "K:C\n",
      "AB/B/ | Bg/c/ =Bd de/c/ | cd ec (f/d/ :|/d/e/d/ ec Bc/c/ B>A BA Bc c/B/B/B/ c2 A3/ B/B/G/A/ Bd | e/2B/2e/ c/2d/2d/2 df/e/f/c/2 c2AB/2=B/2d/2 | dAB d=B/=B/.d/e/ dce2d2 e.dd/.f/e/d/ cd/B/G/ BA B3/B/A/ A/B//2/d/ d cd cd | d2 cA  A>B | dd/c/ d>d | (f/)e/c/d/ fe/c/ ed c2 | d2 d2f:|\n",
      "V:2\n",
      "|: B2c/zB/ d/c/d/2c/2 d/2d/2c/2d/2 | f_ag)b)fe:|\n",
      "f4a2d/2:|B/2A/2f/2d/2e/2d/2A/2A/2=c2|d/2A/2e/2 d4|]\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "# hidden size 64 run 16 epochs\n",
    "for i in range(6):\n",
    "    print(sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X: 10\n",
      "T:Lave Li pelle\n",
      "O:France\n",
      "A:Provence\n",
      "C:Tar.?\n",
      "O:France\n",
      "A:Provence\n",
      "C:Valse parlaul Chervance\n",
      "O:France\n",
      "S:Setich Ma/brigl Fraphigourisup: Usso\n",
      "R:hornpipe\n",
      "H:Dl. fllattave the Mollod, Gary ar de brayche, #25\n",
      "Z:id:hn-jig-66\n",
      "M:6/8\n",
      "K:D\n",
      "DEE FEF|B2d cBA|1 AFE A2A:|2 f2f cAB|AGF DED||\n",
      "<end>\n",
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:10\n",
      "T:Le parse, The\n",
      "T:Ol Hampagher? nu botlow: The Crob\n",
      "R:hornpipe\n",
      "H:Also plas.\n",
      "Z:id:hn-hornpipe-15\n",
      "M:C|\n",
      "K:Amor\n",
      "DAB|AGFG|E,GBdBdg|fedc dAFG|A2AG GAGF|\n",
      "A2AA BAGA|GAGF G2:|\n",
      "K:G|\n",
      "DGEFGAB|dBAGBGAG|EGFG (3BAG FA||\n",
      "<end>\n",
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:14\n",
      "T:Mastrwch Tro\n",
      "R:polkad\n",
      "R:Barande doulle\n",
      "C:Callapti?n (1558)\n",
      "O:France\n",
      "A:Joune\n",
      "R:Marche\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:Bb\n",
      "V:Broux poubriation\n",
      "|:BA | G2AB c2A||\n",
      "<end>\n",
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:22\n",
      "T:Wante\n",
      "R:Roden\n",
      "Z:id:hn-polka-83\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:Ador\n",
      "ag ef|dB AF|ED AB|GB AB|A2 AG|G2 GF|\n",
      "d/e/d AF|EF/A/ B/E/G/E/|FE E/D/d/E|DF/E/ ED|ED A/d/|ea a/g/f/g/|ge gB|1 e>d de:|2 fe dB|A,D2 :|\n",
      "|:E2 Bd/ob|ea ge|d2 B/c/d/e/|fe c=B|AB AB/B/|\n",
      "GA GB/A/|cA A/G/E/E/|DE/F/ DF|F>D EF|D>G FG|AB/A/ AF|1 GE FE:|2 E2 E2:|\n",
      "<end>\n",
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n",
      "<start>\n",
      "X:26\n",
      "T:Gradle \n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2006-11-20\n",
      "Z:Pour toute observation mailto:galo:galousieff@free.frroscBrallo.\n",
      "M:6/8\n",
      "Q:3/4=90\n",
      "K:G\n",
      "BGAB c2ce|fegf gede|cdef ecAG|A2AF G2:|\n",
      "<end>\n",
      "tensor([2.9084e-09, 9.9997e-01, 1.0529e-05, 3.9355e-08, 1.5052e-07, 5.1408e-10,\n",
      "        3.3442e-09, 5.0475e-08, 9.7168e-08, 9.2253e-09, 4.5500e-10, 8.3911e-09,\n",
      "        4.8562e-07, 1.2648e-08, 4.5045e-07, 3.2111e-09, 1.0331e-07, 6.2247e-06,\n",
      "        1.8196e-07, 1.8649e-08, 4.3835e-07, 4.9637e-07, 4.7228e-07, 1.7435e-07,\n",
      "        5.0909e-07, 7.5906e-07, 1.0901e-07, 3.6295e-08, 7.2696e-09, 9.2940e-12,\n",
      "        3.3755e-07, 2.2273e-08, 6.0272e-08, 2.8122e-09, 1.4623e-07, 1.5425e-07,\n",
      "        5.5459e-09, 9.4314e-09, 3.5355e-08, 2.6348e-08, 7.6767e-08, 4.4816e-09,\n",
      "        2.7062e-08, 3.4132e-09, 1.0947e-08, 9.5221e-08, 1.5102e-08, 3.7160e-10,\n",
      "        8.4222e-11, 4.4198e-08, 6.7730e-09, 6.4547e-10, 1.2991e-09, 6.0820e-08,\n",
      "        3.4903e-09, 7.3880e-09, 4.0547e-10, 4.3850e-07, 4.4675e-10, 1.3480e-09,\n",
      "        8.7375e-07, 7.1143e-07, 3.7942e-07, 7.8190e-09, 1.7729e-08, 3.1790e-08,\n",
      "        1.2851e-08, 1.0402e-07, 5.2180e-07, 2.1799e-07, 1.8601e-08, 4.1301e-09,\n",
      "        5.2067e-10, 5.0714e-09, 4.9963e-11, 3.9976e-10, 3.3443e-10, 3.1667e-09,\n",
      "        6.6814e-09, 5.0898e-08, 2.0222e-10, 6.4941e-09, 2.2622e-09, 8.6327e-09,\n",
      "        5.5326e-10, 4.4823e-09, 1.0049e-10, 5.3227e-09, 2.8482e-07, 2.4184e-08,\n",
      "        4.2625e-08, 1.1673e-08, 1.3658e-08, 7.7462e-12, 2.1569e-08],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "X:71\n",
      "T:Lisa mampe, The\n",
      "R:hornpipe\n",
      "D:Mith Frankaig\n",
      "N:machtec Altou:\n",
      "O:France\n",
      "C:Randy Harland <dachesci.frfe.\n",
      "O:France\n",
      "A:Provence\n",
      "C:Trad.\n",
      "R:Ronande\n",
      "H:Scoull played in F7\n",
      "Z:aighneesetttuprar Micillle du harbee \n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-06-12\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "M:3/4\n",
      "Q:3/8Qb3/2 f/2g/2c/2B/2 | c/2c/2 B/2c/2 g/2e/2 | c/2c/2 B/2A/2 A/2G/2 | G/2G/2G/2A/2 EG|A/2A/2 A/2A/2-A/2G/2 | B/2d/4 A2 :|\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "# 20 epochs temp = 1 hidden size = 100\n",
    "for i in range(6):\n",
    "    print(sample_temp(.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
